{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6xjMK7k4PVL"
      },
      "source": [
        "# Tradução Automática Neural com RNNs (Seq2Seq) — Notebook Colab\n",
        "\n",
        "Este notebook implementa um pipeline mínimo de tradução automática inglês→francês inspirado no D2L (Seção 9.5):\n",
        "- Download e pré-processamento do conjunto de dados Tatoeba (ManyThings)\n",
        "- Tokenização e construção de vocabulário (nível de palavra; com tokens reservados)\n",
        "- Preenchimento/truncamento e minibatches\n",
        "- Treinamento de um modelo Seq2Seq simples (GRU)\n",
        "- Avaliação rápida com decodificação gulosa\n",
        "- Experimento de tamanho de vocabulário para diferentes números de exemplos e geração automática de README com respostas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-oryMB74PVO",
        "outputId": "263dc6d0-8b53-4628-fe73-9804e1aad0e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usando dispositivo: cuda\n"
          ]
        }
      ],
      "source": [
        "# Configuração de bibliotecas (Colab)\n",
        "import sys, os, math, time, random, re, html\n",
        "\n",
        "# Instalar PyTorch se necessário (Colab normalmente já possui). D2L não é obrigatório.\n",
        "try:\n",
        "    import torch\n",
        "    import torch.nn as nn\n",
        "    import torch.nn.functional as F\n",
        "    from torch.utils.data import Dataset, DataLoader\n",
        "except Exception as e:\n",
        "    !pip -q install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "    import torch\n",
        "    import torch.nn as nn\n",
        "    import torch.nn.functional as F\n",
        "    from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Usando dispositivo: {DEVICE}\")\n",
        "\n",
        "# Reprodutibilidade\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHyrKen34PVQ",
        "outputId": "84be9056-0c3e-41a7-cc13-1731af423776"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Go.\\tVa !', 'Hi.\\tSalut !', 'Run!\\tCours\\u202f!', 'Run!\\tCourez\\u202f!', 'Who?\\tQui ?']\n"
          ]
        }
      ],
      "source": [
        "# Download e leitura do conjunto Tatoeba Inglês–Francês (ManyThings / espelho D2L)\n",
        "import pathlib, zipfile, urllib.request\n",
        "from typing import Tuple\n",
        "\n",
        "DATA_URL = \"http://d2l-data.s3-accelerate.amazonaws.com/fra-eng.zip\"\n",
        "DATA_DIR = pathlib.Path(\"./data\")\n",
        "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "\n",
        "def download_extract_fra_eng(url: str = DATA_URL, data_dir: pathlib.Path = DATA_DIR) -> pathlib.Path:\n",
        "    \"\"\"Baixa fra-eng.zip e extrai. Retorna o diretório extraído.\"\"\"\n",
        "    zip_path = data_dir / \"fra-eng.zip\"\n",
        "    out_dir = data_dir / \"fra-eng\"\n",
        "    if not out_dir.exists():\n",
        "        if not zip_path.exists():\n",
        "            print(f\"Baixando {url} ...\")\n",
        "            urllib.request.urlretrieve(url, zip_path)\n",
        "        print(\"Extraindo ...\")\n",
        "        with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
        "            zf.extractall(data_dir)\n",
        "    return out_dir\n",
        "\n",
        "\n",
        "def read_data_nmt() -> str:\n",
        "    \"\"\"Lê o 'fra.txt' bruto com linhas Inglês<TAB>Francês.\"\"\"\n",
        "    out_dir = download_extract_fra_eng()\n",
        "    fra_path = out_dir / \"fra.txt\"\n",
        "    with open(fra_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        return f.read()\n",
        "\n",
        "raw_text = read_data_nmt()\n",
        "print(raw_text.splitlines()[:5])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bN52TpT4PVR",
        "outputId": "eb766557-cc5a-4eb1-e3c8-314c3d064622"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['go .\\tva !', 'hi .\\tsalut !', 'run !\\tcours !', 'run !\\tcourez !', 'who ?\\tqui ?']\n"
          ]
        }
      ],
      "source": [
        "# Pré-processamento: normaliza espaços/caixa e insere espaço antes de pontuação (como no D2L)\n",
        "from typing import List\n",
        "\n",
        "def preprocess_nmt(text: str) -> str:\n",
        "    # Substitui espaços não separáveis e converte para minúsculas\n",
        "    text = text.replace('\\u202f', ' ').replace('\\xa0', ' ').lower()\n",
        "    # Insere espaço antes de ,.!? quando precedido por um não-espaço\n",
        "    out_chars = []\n",
        "    for i, ch in enumerate(text):\n",
        "        if ch in ',.!?' and i > 0 and text[i-1] != ' ':\n",
        "            out_chars.append(' ')\n",
        "        out_chars.append(ch)\n",
        "    return ''.join(out_chars)\n",
        "\n",
        "text = preprocess_nmt(raw_text)\n",
        "print(text.splitlines()[:5])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vEBspiby4PVS",
        "outputId": "66424e20-7365-4541-9e2c-39f1af8d2099"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tamanho src_vocab: 10012 tamanho tgt_vocab: 17851\n"
          ]
        }
      ],
      "source": [
        "# Tokenização e vocabulário\n",
        "from collections import Counter\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "\n",
        "def tokenize_nmt(text: str, num_examples: int | None = None) -> tuple[list[list[str]], list[list[str]]]:\n",
        "    src, tgt = [], []\n",
        "    for i, line in enumerate(text.split('\\n')):\n",
        "        if num_examples is not None and i >= num_examples:\n",
        "            break\n",
        "        parts = line.split('\\t')\n",
        "        if len(parts) == 2:\n",
        "            src.append(parts[0].split(' '))\n",
        "            tgt.append(parts[1].split(' '))\n",
        "    return src, tgt\n",
        "\n",
        "class Vocab:\n",
        "    def __init__(self, tokens: List[List[str]], min_freq: int = 1,\n",
        "                 reserved_tokens: List[str] | None = None):\n",
        "        if reserved_tokens is None:\n",
        "            reserved_tokens = []\n",
        "        # Contagem de frequências\n",
        "        counter = Counter()\n",
        "        for line in tokens:\n",
        "            counter.update(line)\n",
        "        # Ordena por frequência e depois alfabeticamente\n",
        "        token_freqs = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\n",
        "        # Tokens especiais\n",
        "        self.pad = '<pad>'\n",
        "        self.bos = '<bos>'\n",
        "        self.eos = '<eos>'\n",
        "        self.unk = '<unk>'\n",
        "        uniq_tokens = [self.pad, self.bos, self.eos, self.unk] + [t for t in reserved_tokens if t not in {self.pad, self.bos, self.eos, self.unk}]\n",
        "        for token, freq in token_freqs:\n",
        "            if freq >= min_freq and token not in uniq_tokens:\n",
        "                uniq_tokens.append(token)\n",
        "        self.idx_to_token = uniq_tokens\n",
        "        self.token_to_idx = {t: i for i, t in enumerate(self.idx_to_token)}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx_to_token)\n",
        "\n",
        "    def __getitem__(self, tokens: List[str] | List[List[str]]):\n",
        "        if not tokens:\n",
        "            return []\n",
        "        if isinstance(tokens[0], list):\n",
        "            return [self.__getitem__(line) for line in tokens]  # type: ignore\n",
        "        return [self.token_to_idx.get(t, self.token_to_idx[self.unk]) for t in tokens]  # type: ignore\n",
        "\n",
        "    def to_tokens(self, indices: List[int] | List[List[int]]):\n",
        "        if not indices:\n",
        "            return []\n",
        "        if isinstance(indices[0], list):\n",
        "            return [self.to_tokens(line) for line in indices]  # type: ignore\n",
        "        return [self.idx_to_token[i] for i in indices]  # type: ignore\n",
        "\n",
        "src_tokens, tgt_tokens = tokenize_nmt(text)\n",
        "src_vocab = Vocab(src_tokens, min_freq=2)\n",
        "tgt_vocab = Vocab(tgt_tokens, min_freq=2)\n",
        "print('tamanho src_vocab:', len(src_vocab), 'tamanho tgt_vocab:', len(tgt_vocab))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2mBUZouX4PVU",
        "outputId": "14f6674d-049c-4839-dd24-42804a5096ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_examples=   300 -> vocab_origem=   102, vocab_destino=   107\n",
            "num_examples=   600 -> vocab_origem=   184, vocab_destino=   201\n",
            "num_examples=  1000 -> vocab_origem=   266, vocab_destino=   321\n",
            "num_examples=  5000 -> vocab_origem=   875, vocab_destino=  1231\n",
            "num_examples= 10000 -> vocab_origem=  1505, vocab_destino=  2252\n",
            "num_examples= 20000 -> vocab_origem=  2459, vocab_destino=  3828\n"
          ]
        }
      ],
      "source": [
        "# Experimento de tamanho de vocabulário para diferentes valores de num_examples (Exercício 1)\n",
        "NUM_EXAMPLES_LIST = [300, 600, 1000, 5000, 10000, 20000]\n",
        "results = []\n",
        "for ne in NUM_EXAMPLES_LIST:\n",
        "    s, t = tokenize_nmt(text, num_examples=ne)\n",
        "    sv = Vocab(s, min_freq=2)\n",
        "    tv = Vocab(t, min_freq=2)\n",
        "    results.append((ne, len(sv), len(tv)))\n",
        "\n",
        "for ne, svsz, tvsz in results:\n",
        "    print(f\"num_examples={ne:6d} -> vocab_origem={svsz:6d}, vocab_destino={tvsz:6d}\")\n",
        "\n",
        "# Salva resultados para o README\n",
        "vocab_experiment_results = results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EpLkJYV34PVW",
        "outputId": "4f9ea1b5-00f5-4cb0-c950-b2c882d8664d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2459, 3828, 20000)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "# Truncar/preencher e DataLoader\n",
        "from typing import Tuple\n",
        "\n",
        "def truncate_pad(line: list[int], num_steps: int, pad_idx: int) -> list[int]:\n",
        "    if len(line) > num_steps:\n",
        "        return line[:num_steps]\n",
        "    return line + [pad_idx] * (num_steps - len(line))\n",
        "\n",
        "\n",
        "def build_array_nmt(lines: list[list[str]], vocab: Vocab, num_steps: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    lines_indices = [vocab[l] for l in lines]\n",
        "    # Acrescenta <eos>\n",
        "    eos_idx = vocab.token_to_idx[vocab.eos]\n",
        "    pad_idx = vocab.token_to_idx[vocab.pad]\n",
        "    lines_indices = [li + [eos_idx] for li in lines_indices]\n",
        "    arr = torch.tensor([truncate_pad(li, num_steps, pad_idx) for li in lines_indices], dtype=torch.long)\n",
        "    valid_len = (arr != pad_idx).int().sum(dim=1)\n",
        "    return arr, valid_len\n",
        "\n",
        "class NMTDataset(Dataset):\n",
        "    def __init__(self, src_lines: list[list[str]], tgt_lines: list[list[str]],\n",
        "                 src_vocab: Vocab, tgt_vocab: Vocab, num_steps: int):\n",
        "        self.src_arr, self.src_valid = build_array_nmt(src_lines, src_vocab, num_steps)\n",
        "        self.tgt_arr, self.tgt_valid = build_array_nmt(tgt_lines, tgt_vocab, num_steps)\n",
        "    def __len__(self):\n",
        "        return self.src_arr.size(0)\n",
        "    def __getitem__(self, idx):\n",
        "        return (self.src_arr[idx], self.src_valid[idx], self.tgt_arr[idx], self.tgt_valid[idx])\n",
        "\n",
        "# Subconjunto pequeno para treinamento mais rápido no Colab\n",
        "NUM_STEPS = 12\n",
        "BATCH_SIZE = 128\n",
        "NUM_EXAMPLES_TRAIN = 20000\n",
        "src_small, tgt_small = tokenize_nmt(text, num_examples=NUM_EXAMPLES_TRAIN)\n",
        "src_vocab = Vocab(src_small, min_freq=2)\n",
        "tgt_vocab = Vocab(tgt_small, min_freq=2)\n",
        "train_ds = NMTDataset(src_small, tgt_small, src_vocab, tgt_vocab, NUM_STEPS)\n",
        "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "len(src_vocab), len(tgt_vocab), len(train_ds)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmkZQiUG4PVY",
        "outputId": "5102ea3d-fe55-49aa-d3da-613a4f203e57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Época 1: perda_por_token=3.8674, ppl=47.82\n",
            "Época 2: perda_por_token=2.5507, ppl=12.82\n",
            "Época 3: perda_por_token=1.7803, ppl=5.93\n",
            "Época 4: perda_por_token=1.2786, ppl=3.59\n",
            "Época 5: perda_por_token=0.9709, ppl=2.64\n",
            "Época 6: perda_por_token=0.7842, ppl=2.19\n",
            "Época 7: perda_por_token=0.6649, ppl=1.94\n",
            "Época 8: perda_por_token=0.5879, ppl=1.80\n",
            "Época 9: perda_por_token=0.5341, ppl=1.71\n",
            "Época 10: perda_por_token=0.4989, ppl=1.65\n",
            "Época 11: perda_por_token=0.4742, ppl=1.61\n",
            "Época 12: perda_por_token=0.4535, ppl=1.57\n",
            "Época 13: perda_por_token=0.4434, ppl=1.56\n",
            "Época 14: perda_por_token=0.4311, ppl=1.54\n",
            "Época 15: perda_por_token=0.4221, ppl=1.53\n",
            "Época 16: perda_por_token=0.4141, ppl=1.51\n",
            "Época 17: perda_por_token=0.4108, ppl=1.51\n",
            "Época 18: perda_por_token=0.4068, ppl=1.50\n",
            "Época 19: perda_por_token=0.3997, ppl=1.49\n",
            "Época 20: perda_por_token=0.3966, ppl=1.49\n",
            "Época 21: perda_por_token=0.3939, ppl=1.48\n",
            "Época 22: perda_por_token=0.3958, ppl=1.49\n",
            "Época 23: perda_por_token=0.3880, ppl=1.47\n",
            "Época 24: perda_por_token=0.3905, ppl=1.48\n",
            "Época 25: perda_por_token=0.3872, ppl=1.47\n",
            "Época 26: perda_por_token=0.3813, ppl=1.46\n",
            "Época 27: perda_por_token=0.3820, ppl=1.47\n",
            "Época 28: perda_por_token=0.3786, ppl=1.46\n",
            "Época 29: perda_por_token=0.3771, ppl=1.46\n",
            "Época 30: perda_por_token=0.3775, ppl=1.46\n",
            "Época 31: perda_por_token=0.3798, ppl=1.46\n",
            "Época 32: perda_por_token=0.3716, ppl=1.45\n",
            "Época 33: perda_por_token=0.3729, ppl=1.45\n",
            "Época 34: perda_por_token=0.3730, ppl=1.45\n",
            "Época 35: perda_por_token=0.3686, ppl=1.45\n",
            "Época 36: perda_por_token=0.3694, ppl=1.45\n",
            "Época 37: perda_por_token=0.3655, ppl=1.44\n",
            "Época 38: perda_por_token=0.3685, ppl=1.45\n",
            "Época 39: perda_por_token=0.3662, ppl=1.44\n",
            "Época 40: perda_por_token=0.3641, ppl=1.44\n",
            "Época 41: perda_por_token=0.3601, ppl=1.43\n",
            "Época 42: perda_por_token=0.3612, ppl=1.44\n",
            "Época 43: perda_por_token=0.3572, ppl=1.43\n",
            "Época 44: perda_por_token=0.3531, ppl=1.42\n",
            "Época 45: perda_por_token=0.3504, ppl=1.42\n",
            "Época 46: perda_por_token=0.3486, ppl=1.42\n",
            "Época 47: perda_por_token=0.3490, ppl=1.42\n",
            "Época 48: perda_por_token=0.3486, ppl=1.42\n",
            "Época 49: perda_por_token=0.3471, ppl=1.42\n",
            "Época 50: perda_por_token=0.3466, ppl=1.41\n",
            "Época 51: perda_por_token=0.3466, ppl=1.41\n",
            "Época 52: perda_por_token=0.3461, ppl=1.41\n",
            "Época 53: perda_por_token=0.3463, ppl=1.41\n",
            "Época 54: perda_por_token=0.3473, ppl=1.42\n",
            "Época 55: perda_por_token=0.3509, ppl=1.42\n",
            "Época 56: perda_por_token=0.3494, ppl=1.42\n",
            "Época 57: perda_por_token=0.3479, ppl=1.42\n",
            "Época 58: perda_por_token=0.3443, ppl=1.41\n",
            "Época 59: perda_por_token=0.3389, ppl=1.40\n",
            "Época 60: perda_por_token=0.3398, ppl=1.40\n",
            "Época 61: perda_por_token=0.3359, ppl=1.40\n",
            "Época 62: perda_por_token=0.3349, ppl=1.40\n",
            "Época 63: perda_por_token=0.3338, ppl=1.40\n",
            "Época 64: perda_por_token=0.3349, ppl=1.40\n",
            "Época 65: perda_por_token=0.3306, ppl=1.39\n",
            "Época 66: perda_por_token=0.3344, ppl=1.40\n",
            "Época 67: perda_por_token=0.3385, ppl=1.40\n",
            "Época 68: perda_por_token=0.3321, ppl=1.39\n",
            "Época 69: perda_por_token=0.3324, ppl=1.39\n",
            "Época 70: perda_por_token=0.3332, ppl=1.40\n",
            "Época 71: perda_por_token=0.3322, ppl=1.39\n",
            "Época 72: perda_por_token=0.3288, ppl=1.39\n",
            "Época 73: perda_por_token=0.3272, ppl=1.39\n",
            "Época 74: perda_por_token=0.3274, ppl=1.39\n",
            "Época 75: perda_por_token=0.3284, ppl=1.39\n",
            "Época 76: perda_por_token=0.3301, ppl=1.39\n",
            "Época 77: perda_por_token=0.3277, ppl=1.39\n",
            "Época 78: perda_por_token=0.3276, ppl=1.39\n",
            "Época 79: perda_por_token=0.3246, ppl=1.38\n",
            "Época 80: perda_por_token=0.3265, ppl=1.39\n",
            "Época 81: perda_por_token=0.3232, ppl=1.38\n",
            "Época 82: perda_por_token=0.3200, ppl=1.38\n",
            "Época 83: perda_por_token=0.3225, ppl=1.38\n",
            "Época 84: perda_por_token=0.3181, ppl=1.37\n",
            "Época 85: perda_por_token=0.3163, ppl=1.37\n",
            "Época 86: perda_por_token=0.3144, ppl=1.37\n",
            "Época 87: perda_por_token=0.3141, ppl=1.37\n",
            "Época 88: perda_por_token=0.3185, ppl=1.38\n",
            "Época 89: perda_por_token=0.3205, ppl=1.38\n",
            "Época 90: perda_por_token=0.3238, ppl=1.38\n",
            "Época 91: perda_por_token=0.3306, ppl=1.39\n",
            "Época 92: perda_por_token=0.3336, ppl=1.40\n",
            "Época 93: perda_por_token=0.3343, ppl=1.40\n",
            "Época 94: perda_por_token=0.3304, ppl=1.39\n",
            "Época 95: perda_por_token=0.3253, ppl=1.38\n",
            "Época 96: perda_por_token=0.3184, ppl=1.37\n",
            "Época 97: perda_por_token=0.3207, ppl=1.38\n",
            "Época 98: perda_por_token=0.3161, ppl=1.37\n",
            "Época 99: perda_por_token=0.3102, ppl=1.36\n",
            "Época 100: perda_por_token=0.3094, ppl=1.36\n",
            "Época 101: perda_por_token=0.3049, ppl=1.36\n",
            "Época 102: perda_por_token=0.3017, ppl=1.35\n",
            "Época 103: perda_por_token=0.3055, ppl=1.36\n",
            "Época 104: perda_por_token=0.3027, ppl=1.35\n",
            "Época 105: perda_por_token=0.3059, ppl=1.36\n",
            "Época 106: perda_por_token=0.3090, ppl=1.36\n",
            "Época 107: perda_por_token=0.3106, ppl=1.36\n",
            "Época 108: perda_por_token=0.3132, ppl=1.37\n",
            "Época 109: perda_por_token=0.3194, ppl=1.38\n",
            "Época 110: perda_por_token=0.3296, ppl=1.39\n",
            "Época 111: perda_por_token=0.3331, ppl=1.40\n",
            "Época 112: perda_por_token=0.3405, ppl=1.41\n",
            "Época 113: perda_por_token=0.3383, ppl=1.40\n",
            "Época 114: perda_por_token=0.3319, ppl=1.39\n",
            "Época 115: perda_por_token=0.3226, ppl=1.38\n",
            "Época 116: perda_por_token=0.3181, ppl=1.37\n",
            "Época 117: perda_por_token=0.3126, ppl=1.37\n",
            "Época 118: perda_por_token=0.3047, ppl=1.36\n",
            "Época 119: perda_por_token=0.3063, ppl=1.36\n",
            "Época 120: perda_por_token=0.3009, ppl=1.35\n",
            "Época 121: perda_por_token=0.3003, ppl=1.35\n",
            "Época 122: perda_por_token=0.2957, ppl=1.34\n",
            "Época 123: perda_por_token=0.2934, ppl=1.34\n",
            "Época 124: perda_por_token=0.2950, ppl=1.34\n",
            "Época 125: perda_por_token=0.2940, ppl=1.34\n",
            "Época 126: perda_por_token=0.2973, ppl=1.35\n",
            "Época 127: perda_por_token=0.2972, ppl=1.35\n",
            "Época 128: perda_por_token=0.2993, ppl=1.35\n",
            "Época 129: perda_por_token=0.3044, ppl=1.36\n",
            "Época 130: perda_por_token=0.3128, ppl=1.37\n",
            "Época 131: perda_por_token=0.3190, ppl=1.38\n",
            "Época 132: perda_por_token=0.3277, ppl=1.39\n",
            "Época 133: perda_por_token=0.3340, ppl=1.40\n",
            "Época 134: perda_por_token=0.3332, ppl=1.40\n",
            "Época 135: perda_por_token=0.3318, ppl=1.39\n",
            "Época 136: perda_por_token=0.3255, ppl=1.38\n",
            "Época 137: perda_por_token=0.3219, ppl=1.38\n",
            "Época 138: perda_por_token=0.3148, ppl=1.37\n",
            "Época 139: perda_por_token=0.3107, ppl=1.36\n",
            "Época 140: perda_por_token=0.3065, ppl=1.36\n",
            "Época 141: perda_por_token=0.3023, ppl=1.35\n",
            "Época 142: perda_por_token=0.2987, ppl=1.35\n",
            "Época 143: perda_por_token=0.2932, ppl=1.34\n",
            "Época 144: perda_por_token=0.2932, ppl=1.34\n",
            "Época 145: perda_por_token=0.2902, ppl=1.34\n",
            "Época 146: perda_por_token=0.2865, ppl=1.33\n",
            "Época 147: perda_por_token=0.2858, ppl=1.33\n",
            "Época 148: perda_por_token=0.2850, ppl=1.33\n",
            "Época 149: perda_por_token=0.2882, ppl=1.33\n",
            "Época 150: perda_por_token=0.2875, ppl=1.33\n"
          ]
        }
      ],
      "source": [
        "# Seq2Seq simples com GRU (codificador-decodificador) e decodificação gulosa\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size: int, embed_size: int, hidden_size: int, num_layers: int = 1, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
        "        self.gru = nn.GRU(embed_size, hidden_size, num_layers=num_layers, batch_first=True, dropout=dropout if num_layers>1 else 0.0)\n",
        "    def forward(self, x, valid_len):\n",
        "        emb = self.embed(x)\n",
        "        # Opcional: poderíamos empacotar sequências com padding, mas manteremos simples\n",
        "        outputs, hidden = self.gru(emb)\n",
        "        return outputs, hidden\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size: int, embed_size: int, hidden_size: int, num_layers: int = 1, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
        "        self.gru = nn.GRU(embed_size, hidden_size, num_layers=num_layers, batch_first=True, dropout=dropout if num_layers>1 else 0.0)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "    def forward(self, x, hidden):\n",
        "        emb = self.embed(x)\n",
        "        out, hidden = self.gru(emb, hidden)\n",
        "        logits = self.fc(out)\n",
        "        return logits, hidden\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, enc: Encoder, dec: Decoder, tgt_pad_idx: int):\n",
        "        super().__init__()\n",
        "        self.enc = enc\n",
        "        self.dec = dec\n",
        "        self.tgt_pad_idx = tgt_pad_idx\n",
        "    def forward(self, src, src_valid_len, tgt):\n",
        "        _, hidden = self.enc(src, src_valid_len)\n",
        "        # Teacher forcing: alimenta os tokens verdadeiros deslocados (preprende <bos>)\n",
        "        logits, _ = self.dec(tgt[:, :-1], hidden)\n",
        "        return logits\n",
        "\n",
        "# Construir modelo\n",
        "EMBED_SIZE = 128\n",
        "HIDDEN_SIZE = 256\n",
        "enc = Encoder(len(src_vocab), EMBED_SIZE, HIDDEN_SIZE)\n",
        "dec = Decoder(len(tgt_vocab), EMBED_SIZE, HIDDEN_SIZE)\n",
        "model = Seq2Seq(enc, dec, tgt_pad_idx=tgt_vocab.token_to_idx[tgt_vocab.pad]).to(DEVICE)\n",
        "\n",
        "# Função de perda e otimizador\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=tgt_vocab.token_to_idx[tgt_vocab.pad])\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=3e-3)\n",
        "\n",
        "# Preparar função de criação de entradas/saídas do decodificador\n",
        "BOS_IDX = tgt_vocab.token_to_idx[tgt_vocab.bos]\n",
        "EOS_IDX = tgt_vocab.token_to_idx[tgt_vocab.eos]\n",
        "PAD_IDX = tgt_vocab.token_to_idx[tgt_vocab.pad]\n",
        "\n",
        "def make_decoder_inputs_targets(tgt_batch: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
        "    # tgt_batch forma: (B, T). Entrada: [<bos>, y_0, ..., y_{T-2}] e alvo: [y_0, ..., y_{T-2}, y_{T-1}]\n",
        "    B, T = tgt_batch.size()\n",
        "    bos_col = torch.full((B, 1), BOS_IDX, dtype=tgt_batch.dtype, device=tgt_batch.device)\n",
        "    dec_in = torch.cat([bos_col, tgt_batch[:, :-1]], dim=1)\n",
        "    dec_out = tgt_batch\n",
        "    return dec_in, dec_out\n",
        "\n",
        "# Laço de treinamento\n",
        "EPOCHS = 150\n",
        "model.train()\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    total_loss = 0.0\n",
        "    total_tokens = 0\n",
        "    for src_b, src_len_b, tgt_b, tgt_len_b in train_dl:\n",
        "        src_b = src_b.to(DEVICE)\n",
        "        tgt_b = tgt_b.to(DEVICE)\n",
        "        dec_in, dec_out = make_decoder_inputs_targets(tgt_b)\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(src_b, src_len_b.to(DEVICE), dec_in)\n",
        "        # Alinha formas para a perda: logits (B, T-1, V) vs alvos (B, T-1)\n",
        "        loss = criterion(logits.reshape(-1, logits.size(-1)), dec_out[:, 1:].reshape(-1))\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        # Acumula\n",
        "        ntokens = (dec_out[:, 1:] != PAD_IDX).sum().item()\n",
        "        total_loss += loss.item() * ntokens\n",
        "        total_tokens += ntokens\n",
        "    ppl = math.exp(total_loss / max(1, total_tokens))\n",
        "    print(f\"Época {epoch}: perda_por_token={(total_loss/total_tokens):.4f}, ppl={ppl:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvBPiZcf4PVb",
        "outputId": "e76b0acc-0e22-4b8d-d6a1-0fdf4f0bbe21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EN: go .\n",
            "FR(verdadeiro): va !\n",
            "FR(previsto): ! y . suis .\n",
            "---\n",
            "EN: wow !\n",
            "FR(verdadeiro): ça alors !\n",
            "FR(previsto): alors ! ! de ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !\n",
            "---\n",
            "EN: stop !\n",
            "FR(verdadeiro): stop !\n",
            "FR(previsto): ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !\n",
            "---\n",
            "EN: i try .\n",
            "FR(verdadeiro): j'essaye .\n",
            "FR(previsto): . .\n",
            "---\n",
            "EN: cheers !\n",
            "FR(verdadeiro): tchin-tchin !\n",
            "FR(previsto): ! ! votre ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !\n",
            "---\n"
          ]
        }
      ],
      "source": [
        "# Decodificação gulosa para avaliação qualitativa rápida\n",
        "@torch.no_grad()\n",
        "def translate_greedy(src_sentence: str, max_len: int = 20) -> str:\n",
        "    # Pré-processa e tokeniza a frase de entrada (apenas Inglês)\n",
        "    s = preprocess_nmt(src_sentence)\n",
        "    s_tokens = s.strip().split(' ')\n",
        "    src_idx = torch.tensor([truncate_pad(src_vocab[s_tokens] + [src_vocab.token_to_idx[src_vocab.eos]], NUM_STEPS, src_vocab.token_to_idx[src_vocab.pad])], dtype=torch.long).to(DEVICE)\n",
        "    _, hidden = model.enc(src_idx, torch.tensor([len(s_tokens)+1]))\n",
        "    # Inicia com <bos>\n",
        "    cur = torch.tensor([[BOS_IDX]], dtype=torch.long, device=DEVICE)\n",
        "    out_tokens: list[int] = []\n",
        "    for _ in range(max_len):\n",
        "        logits, hidden = model.dec(cur, hidden)\n",
        "        next_token = logits[:, -1].argmax(dim=-1)\n",
        "        token_id = next_token.item()\n",
        "        if token_id == EOS_IDX or token_id == PAD_IDX:\n",
        "            break\n",
        "        out_tokens.append(token_id)\n",
        "        cur = torch.tensor([[token_id]], dtype=torch.long, device=DEVICE)\n",
        "    return ' '.join(tgt_vocab.to_tokens(out_tokens))\n",
        "\n",
        "# Testa algumas amostras curtas do conjunto de dados\n",
        "for i in [0, 5, 10, 20, 30]:\n",
        "    en = ' '.join(src_small[i])\n",
        "    fr = ' '.join(tgt_small[i])\n",
        "    pred = translate_greedy(en)\n",
        "    print(f\"EN: {en}\\nFR(verdadeiro): {fr}\\nFR(previsto): {pred}\\n---\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uaLb3tma4PVd",
        "outputId": "cb599f3a-e6cb-44e8-e265-5cc1fd9fe20a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "README.md written.\n"
          ]
        }
      ],
      "source": [
        "# Auto-generate README.md with documentation and exercise answers\n",
        "import textwrap, json\n",
        "\n",
        "EXERCISE_1_TEXT = \"\"\"\n",
        "Pergunta: Tente valores diferentes do argumento num_examples na função load_data_nmt.\n",
        "Como isso afeta os tamanhos do vocabulário do idioma de origem e do idioma de destino?\n",
        "\n",
        "Resposta (baseada nos resultados deste notebook):\n",
        "\"\"\"\n",
        "\n",
        "lines = [f\"- num_examples={ne}: src_vocab={sv}, tgt_vocab={tv}\" for (ne, sv, tv) in vocab_experiment_results]\n",
        "ex1_answer = EXERCISE_1_TEXT + \"\\n\" + \"\\n\".join(lines) + \"\\n\\n\" + \\\n",
        "    \"Conclusão: conforme aumentamos num_examples, ambos os vocabulários crescem monotonicamente (ou quase),\\n\" \\\n",
        "    \"pois mais sentenças expõem mais tipos de palavras. As taxas de crescimento diferem por idioma\\n\" \\\n",
        "    \"devido a diferenças morfológicas e distribuição de tokens no corpus.\"\n",
        "\n",
        "EXERCISE_2_TEXT = \"\"\"\n",
        "Pergunta: O texto em alguns idiomas, como chinês e japonês, não tem indicadores de limite de palavras.\n",
        "A tokenização em nível de palavra ainda é uma boa ideia para esses casos? Por que ou por que não?\n",
        "\n",
        "Resposta:\n",
        "Não. Em línguas sem separadores explícitos de palavras (p.ex., chinês, japonês), a tokenização em nível de palavra\n",
        "exige uma etapa externa de segmentação que é ruidosa e dependente de dicionário. Isso pode introduzir erros sistêmicos\n",
        "no treinamento. Em vez disso, tokenização em nível de subpalavra (BPE/WordPiece/Unigram, como em Sennrich et al., 2016; Kudo, 2018)\n",
        "ou mesmo nível de caractere pode ser preferível, pois lida melhor com morfologia rica e OOV.\n",
        "\n",
        "Referências:\n",
        "- D2L 9.5 destaca que o vocabulário em nível de palavra cresce muito, e sugere técnicas de tokenização mais avançadas.\n",
        "- Sennrich, Haddow, Birch (2016): Neural Machine Translation of Rare Words with Subword Units.\n",
        "- Kudo (2018): Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates.\n",
        "- Devlin et al. (2019): BERT uses WordPiece (subword) tokenization, amplamente adotado em PLN.\n",
        "\"\"\"\n",
        "\n",
        "readme = f\"\"\"\n",
        "# Neural Machine Translation with RNNs (Seq2Seq)\n",
        "\n",
        "Este projeto implementa um pipeline simples de tradução automática (inglês→francês) inspirado em D2L Seção 9.5.\n",
        "\n",
        "## Conteúdo\n",
        "- `NMT_Seq2Seq_Colab.ipynb`: Notebook Colab com:\n",
        "  - Download e pré-processamento (ManyThings/Tatoeba, fra-eng.zip)\n",
        "  - Tokenização e vocabulários com tokens especiais (<pad>, <bos>, <eos>, <unk>)\n",
        "  - Truncamento/preenchimento e minibatches\n",
        "  - Modelo Seq2Seq (Encoder/Decoder GRU) com treinamento rápido\n",
        "  - Decodificação gulosa para avaliação qualitativa\n",
        "  - Experimento de tamanho de vocabulário variando `num_examples`\n",
        "\n",
        "## Como executar (Colab)\n",
        "1. Abra o notebook no Google Colab e selecione GPU (opcional).\n",
        "2. Execute todas as células em ordem. O último bloco gera este README automaticamente.\n",
        "\n",
        "## Resultados (resumo)\n",
        "- Perplexidade por época exibida no treinamento (ver notebook).\n",
        "- Traduções de exemplo impressas (qualitativas).\n",
        "\n",
        "## Exercícios (D2L 9.5.7)\n",
        "\n",
        "### 1) Variação de `num_examples` e tamanhos de vocabulário\n",
        "{ex1_answer}\n",
        "\n",
        "### 2) Tokenização em idiomas sem separadores de palavras (chinês/japonês)\n",
        "{EXERCISE_2_TEXT}\n",
        "\"\"\"\n",
        "\n",
        "with open(\"README.md\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(textwrap.dedent(readme))\n",
        "\n",
        "print(\"README.md written.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}